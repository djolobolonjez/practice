{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSI:ML7 - TF-IDF (practice).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYSmgs7WvD/26EK9xzxrqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djolobolonjez/practice/blob/master/PSI_ML7_TF_IDF_(practice).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization"
      ],
      "metadata": {
        "id": "xvA9kHNoVN5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdi8ojiviv5Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import sys\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "\n",
        "from nltk.stem import SnowballStemmer \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "wordmap = {}\n",
        "sentmap = {}\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "docnum = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File opening for term frequency"
      ],
      "metadata": {
        "id": "SSPtJa51VV7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def file_open(fl):\n",
        "  with open(fl, 'r') as f:\n",
        "     for line in f:\n",
        "       words = word_tokenize(line)\n",
        "       for w in words:\n",
        "         term = stemmer.stem(w.lower())\n",
        "         if (term not in wordmap and term.isalnum()):\n",
        "           wordmap[term] = 1\n",
        "         else:\n",
        "           if(term.isalnum()):\n",
        "            wordmap[term] += 1"
      ],
      "metadata": {
        "id": "HZlog3Fbjgbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing of the input"
      ],
      "metadata": {
        "id": "BusKDXu3Vhic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory, pth = input().split(\" \")\n",
        "path = os.path.split(pth)\n",
        "title = path[-1]\n",
        "os.chdir(path[0])\n",
        "file_open(title)"
      ],
      "metadata": {
        "id": "6pEEKvGsiy0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document frequency"
      ],
      "metadata": {
        "id": "aBeTg2YoVplT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_count(fl):\n",
        "  with open(fl, 'r') as f:\n",
        "    text = f.read()\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    for i in range(len(words)):\n",
        "      words[i] = stemmer.stem(words[i].lower())\n",
        "    for key in stems:\n",
        "      if key in words:\n",
        "        stems[key] += 1"
      ],
      "metadata": {
        "id": "wMJXCzdBzKQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling nested directories"
      ],
      "metadata": {
        "id": "7hln-ap6VsTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dir_tree(dir):\n",
        "  global docnum\n",
        "  stack = [dir]\n",
        "\n",
        "  while stack:\n",
        "    pushed = 0\n",
        "    dir = stack.pop()\n",
        "    files = os.listdir(dir)\n",
        "    os.chdir(dir)\n",
        "    for file in files:\n",
        "      if not file.endswith(\".txt\"):\n",
        "        pushed += 1\n",
        "        stack.append(file)\n",
        "        \n",
        "      else:\n",
        "        docnum += 1\n",
        "        df_count(file)\n",
        "    if not pushed:\n",
        "      path_parent = os.path.dirname(os.getcwd())\n",
        "      os.chdir(path_parent)"
      ],
      "metadata": {
        "id": "8oybKHqIdqfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparation for counting DF"
      ],
      "metadata": {
        "id": "LH9NeTbgWGV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stems = wordmap.copy()\n",
        "for key in stems:\n",
        "  stems[key] = 0\n",
        "\n",
        "for roots, dirs, files in os.walk(directory):\n",
        "  for dir in dirs:\n",
        "    os.chdir(directory)\n",
        "    if not dir.startswith(\".\"):\n",
        "      dir_tree(dir)\n",
        "  break"
      ],
      "metadata": {
        "id": "dXDK2k4zsIfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversion to IDF from DF"
      ],
      "metadata": {
        "id": "0PvS3jqgWJZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_idf(stems):\n",
        "  for key in stems:\n",
        "    stems[key] = math.log((docnum/stems[key]), 10)"
      ],
      "metadata": {
        "id": "kzcstwrrDp2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF "
      ],
      "metadata": {
        "id": "T1Dq-sUPWL6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf(wordmap, stems, result):\n",
        "  wm = wordmap.copy()\n",
        "  stm = stems.copy()\n",
        "  for i in range(10):\n",
        "    max = 0\n",
        "    max_key = \"\"\n",
        "    for key in stm:\n",
        "      tf_idf = stm[key]*wm[key]\n",
        "      if tf_idf > max:\n",
        "        max = tf_idf\n",
        "        max_key = key\n",
        "    \n",
        "    result.append([max_key, max])\n",
        "    wm.pop(max_key, None)\n",
        "    stm.pop(max_key, None)\n",
        "    max = 0\n",
        "    max_key = \"\""
      ],
      "metadata": {
        "id": "MWb6_Koj0uyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting some initial TF-IDF results"
      ],
      "metadata": {
        "id": "D5fXL8QgXM8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "to_idf(stems)\n",
        "calculate_tfidf(wordmap, stems, result)\n",
        "\n",
        "for i in range(len(result)-1):\n",
        "  for j in range(i, len(result)):\n",
        "    if(result[i][-1] == result[j][-1]):\n",
        "      if(result[i][0] > result[j][0]):\n",
        "        tmp = result[i][0]\n",
        "        result[i][0] = result[j][0]\n",
        "        result[j][0] = tmp"
      ],
      "metadata": {
        "id": "7ibLgnVx2vM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF of sentences"
      ],
      "metadata": {
        "id": "cT_rq976WwjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_tfidf(words, sent):\n",
        "  results = []\n",
        "  wm = wordmap.copy()\n",
        "  stm = stems.copy()\n",
        "  added = []\n",
        "  found = True\n",
        "  for i in range(10):\n",
        "    max = 0\n",
        "    max_key = \"\"\n",
        "    rmv = \"\"\n",
        "    \n",
        "    for word in words:\n",
        "\n",
        "      wrd = stemmer.stem(word.lower())\n",
        "      if wrd in added:\n",
        "        max = stems[wrd]*wordmap[wrd]\n",
        "        max_key = wrd\n",
        "        rmv = word\n",
        "        break\n",
        "      else:\n",
        "        tf_idf = stm[wrd]*wm[wrd]\n",
        "        if tf_idf > max:\n",
        "          max = tf_idf\n",
        "          max_key = wrd\n",
        "          rmv = word\n",
        "    \n",
        "    if rmv != \"\": \n",
        "      words.remove(rmv)\n",
        "      added.append(max_key)\n",
        "      results.append([max_key, max])\n",
        "      wm.pop(max_key, None)\n",
        "      stm.pop(max_key, None)\n",
        "    max = 0\n",
        "    max_key = \"\"\n",
        "    rmv = \"\"\n",
        "\n",
        "  stfidf = 0\n",
        "  for res in results:\n",
        "    stfidf += res[-1]\n",
        "  sentmap[sent] = stfidf\n"
      ],
      "metadata": {
        "id": "JLV0r6vZuX2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparation for sentence tf-idf calculation"
      ],
      "metadata": {
        "id": "g8GOorFOW0DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def second_task(fl):\n",
        "  \n",
        "  sts = []\n",
        "  with open(fl, 'r') as f:\n",
        "    content = f.read()\n",
        "    sentences = sent_tokenize(content)\n",
        "    sts = list(sentences)\n",
        "    for sent in sentences:\n",
        "      words = word_tokenize(sent)\n",
        "      words = [word for word in words if word.isalnum()]\n",
        "      sent_tfidf(words, sent)\n",
        "  \n",
        "  return sts, dict(sorted(sentmap.items(), key = lambda item: item[1], reverse = True))"
      ],
      "metadata": {
        "id": "RLFs2YyEo2A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting and printing out final data"
      ],
      "metadata": {
        "id": "WDQOhlcOW_6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terms = []\n",
        "for i in range(len(result)):\n",
        "  terms.append(result[i][0])\n",
        "os.chdir(path[0])\n",
        "sts, secondmap = second_task(title)\n",
        "\n",
        "sents = []\n",
        "sentences = []\n",
        "for key in secondmap.keys():\n",
        "  sents.append(key)\n",
        "  \n",
        "for i in range(5):\n",
        "  sentences.append(sents[i])\n",
        "for i in range(len(sentences)-1):\n",
        "  for j in range(i, len(sentences)):\n",
        "    if(secondmap[sentences[i]] == secondmap[sentences[j]]):\n",
        "      if(sts.index(sentences[i]) > sts.index(sentences[j])):\n",
        "        tmp = sentences[i]\n",
        "        sentences[i] = sentences[j]\n",
        "        sentences[j] = tmp\n",
        "print(*terms, sep = \", \")\n",
        "print(*sentences, sep = \" \")"
      ],
      "metadata": {
        "id": "cgVkOO1IIthU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}