{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSI:ML7 - TF-IDF (practice).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYSmgs7WvD/26EK9xzxrqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djolobolonjez/practice/blob/master/PSI_ML7_TF_IDF_(practice).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization"
      ],
      "metadata": {
        "id": "xvA9kHNoVN5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdi8ojiviv5Q",
        "outputId": "c0299e9d-ecc2-4fe6-b298-acf04955b000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import sys\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "\n",
        "from nltk.stem import SnowballStemmer \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "wordmap = {}\n",
        "sentmap = {}\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "docnum = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File opening for term frequency"
      ],
      "metadata": {
        "id": "SSPtJa51VV7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def file_open(fl):\n",
        "  with open(fl, 'r') as f:\n",
        "     for line in f:\n",
        "       words = word_tokenize(line)\n",
        "       for w in words:\n",
        "         term = stemmer.stem(w.lower())\n",
        "         if (term not in wordmap and term.isalnum()):\n",
        "           wordmap[term] = 1\n",
        "         else:\n",
        "           if(term.isalnum()):\n",
        "            wordmap[term] += 1"
      ],
      "metadata": {
        "id": "HZlog3Fbjgbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing of the input"
      ],
      "metadata": {
        "id": "BusKDXu3Vhic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory, pth = input().split(\" \")\n",
        "path = os.path.split(pth)\n",
        "title = path[-1]\n",
        "os.chdir(path[0])\n",
        "file_open(title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pEEKvGsiy0J",
        "outputId": "95d82c0b-fc6c-4770-b33d-0f9b73a22be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/corpus,/corpus/skiing/History of skiing.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document frequency"
      ],
      "metadata": {
        "id": "aBeTg2YoVplT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_count(fl):\n",
        "  with open(fl, 'r') as f:\n",
        "    text = f.read()\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    for i in range(len(words)):\n",
        "      words[i] = stemmer.stem(words[i].lower())\n",
        "    for key in stems:\n",
        "      if key in words:\n",
        "        stems[key] += 1"
      ],
      "metadata": {
        "id": "wMJXCzdBzKQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling nested directories"
      ],
      "metadata": {
        "id": "7hln-ap6VsTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dir_tree(dir):\n",
        "  global docnum\n",
        "  stack = [dir]\n",
        "\n",
        "  while stack:\n",
        "    pushed = 0\n",
        "    dir = stack.pop()\n",
        "    files = os.listdir(dir)\n",
        "    os.chdir(dir)\n",
        "    for file in files:\n",
        "      if not file.endswith(\".txt\"):\n",
        "        pushed += 1\n",
        "        stack.append(file)\n",
        "        \n",
        "      else:\n",
        "        docnum += 1\n",
        "        df_count(file)\n",
        "    if not pushed:\n",
        "      path_parent = os.path.dirname(os.getcwd())\n",
        "      os.chdir(path_parent)"
      ],
      "metadata": {
        "id": "8oybKHqIdqfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparation for counting DF"
      ],
      "metadata": {
        "id": "LH9NeTbgWGV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stems = wordmap.copy()\n",
        "for key in stems:\n",
        "  stems[key] = 0\n",
        "\n",
        "for roots, dirs, files in os.walk(directory):\n",
        "  for dir in dirs:\n",
        "    os.chdir(directory)\n",
        "    if not dir.startswith(\".\"):\n",
        "      dir_tree(dir)\n",
        "  break"
      ],
      "metadata": {
        "id": "dXDK2k4zsIfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversion to IDF from DF"
      ],
      "metadata": {
        "id": "0PvS3jqgWJZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_idf(stems):\n",
        "  for key in stems:\n",
        "    stems[key] = math.log((docnum/stems[key]), 10)"
      ],
      "metadata": {
        "id": "kzcstwrrDp2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF "
      ],
      "metadata": {
        "id": "T1Dq-sUPWL6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf(wordmap, stems, result):\n",
        "  wm = wordmap.copy()\n",
        "  stm = stems.copy()\n",
        "  for i in range(10):\n",
        "    max = 0\n",
        "    max_key = \"\"\n",
        "    for key in stm:\n",
        "      tf_idf = stm[key]*wm[key]\n",
        "      if tf_idf > max:\n",
        "        max = tf_idf\n",
        "        max_key = key\n",
        "    \n",
        "    result.append([max_key, max])\n",
        "    wm.pop(max_key, None)\n",
        "    stm.pop(max_key, None)\n",
        "    max = 0\n",
        "    max_key = \"\""
      ],
      "metadata": {
        "id": "MWb6_Koj0uyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting some initial TF-IDF results"
      ],
      "metadata": {
        "id": "D5fXL8QgXM8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "to_idf(stems)\n",
        "calculate_tfidf(wordmap, stems, result)\n",
        "\n",
        "for i in range(len(result)-1):\n",
        "  for j in range(i, len(result)):\n",
        "    if(result[i][-1] == result[j][-1]):\n",
        "      if(result[i][0] > result[j][0]):\n",
        "        tmp = result[i][0]\n",
        "        result[i][0] = result[j][0]\n",
        "        result[j][0] = tmp"
      ],
      "metadata": {
        "id": "7ibLgnVx2vM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF of sentences"
      ],
      "metadata": {
        "id": "cT_rq976WwjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_tfidf(words, sent):\n",
        "  results = []\n",
        "  wm = wordmap.copy()\n",
        "  stm = stems.copy()\n",
        "  added = []\n",
        "  found = True\n",
        "  for i in range(10):\n",
        "    max = 0\n",
        "    max_key = \"\"\n",
        "    rmv = \"\"\n",
        "    \n",
        "    for word in words:\n",
        "\n",
        "      wrd = stemmer.stem(word.lower())\n",
        "      if wrd in added:\n",
        "        max = stems[wrd]*wordmap[wrd]\n",
        "        max_key = wrd\n",
        "        rmv = word\n",
        "        break\n",
        "      else:\n",
        "        tf_idf = stm[wrd]*wm[wrd]\n",
        "        if tf_idf > max:\n",
        "          max = tf_idf\n",
        "          max_key = wrd\n",
        "          rmv = word\n",
        "    \n",
        "    if rmv != \"\": \n",
        "      words.remove(rmv)\n",
        "      added.append(max_key)\n",
        "      results.append([max_key, max])\n",
        "      wm.pop(max_key, None)\n",
        "      stm.pop(max_key, None)\n",
        "    max = 0\n",
        "    max_key = \"\"\n",
        "    rmv = \"\"\n",
        "\n",
        "  stfidf = 0\n",
        "  for res in results:\n",
        "    stfidf += res[-1]\n",
        "  sentmap[sent] = stfidf\n"
      ],
      "metadata": {
        "id": "JLV0r6vZuX2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparation for sentence tf-idf calculation"
      ],
      "metadata": {
        "id": "g8GOorFOW0DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def second_task(fl):\n",
        "  \n",
        "  sts = []\n",
        "  with open(fl, 'r') as f:\n",
        "    content = f.read()\n",
        "    sentences = sent_tokenize(content)\n",
        "    sts = list(sentences)\n",
        "    for sent in sentences:\n",
        "      words = word_tokenize(sent)\n",
        "      words = [word for word in words if word.isalnum()]\n",
        "      sent_tfidf(words, sent)\n",
        "  \n",
        "  return sts, dict(sorted(sentmap.items(), key = lambda item: item[1], reverse = True))"
      ],
      "metadata": {
        "id": "RLFs2YyEo2A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting and printing out final data"
      ],
      "metadata": {
        "id": "WDQOhlcOW_6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terms = []\n",
        "for i in range(len(result)):\n",
        "  terms.append(result[i][0])\n",
        "os.chdir(path[0])\n",
        "sts, secondmap = second_task(title)\n",
        "\n",
        "sents = []\n",
        "sentences = []\n",
        "for key in secondmap.keys():\n",
        "  sents.append(key)\n",
        "  \n",
        "for i in range(5):\n",
        "  sentences.append(sents[i])\n",
        "for i in range(len(sentences)-1):\n",
        "  for j in range(i, len(sentences)):\n",
        "    if(secondmap[sentences[i]] == secondmap[sentences[j]]):\n",
        "      if(sts.index(sentences[i]) > sts.index(sentences[j])):\n",
        "        tmp = sentences[i]\n",
        "        sentences[i] = sentences[j]\n",
        "        sentences[j] = tmp\n",
        "print(*terms, sep = \", \")\n",
        "print(*sentences, sep = \" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgVkOO1IIthU",
        "outputId": "9e390309-9f2c-426f-a94b-8b9409775b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ski, sami, norway, bce, norwegian, strap, skier, wax, club, bind\n",
            "The Sami also have their own words for \"skis\" and \"skiing\": for example, the Lule Sami word for \"ski\" is sabek and skis are called sabega. As skiing became more specialized, so too did ski boots, leading to the splitting of designs between those for alpine skiing and cross-country skiing. Finnish has its own ancient words for skis and skiing: \"ski\" is suksi and \"skiing\" is hiihtää. For instance details of military ski exercises in the Danish-Norwegian army from 1767 are retained: Military races and exercises included downhill in rough terrain, target practice while skiing downhill, and 3 km cross-country skiing with full military backpack. The bottom of the short ski was either plain or covered with animal skin to aid this use, while the long ski supporting the weight of the skier was treated with animal fat in similar manner to modern ski waxing.\n"
          ]
        }
      ]
    }
  ]
}