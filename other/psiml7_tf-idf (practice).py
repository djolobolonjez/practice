# -*- coding: utf-8 -*-
"""PSI:ML7 - TF-IDF (practice).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UTvZbGFF_lVXLFBtUJ0FXjixFNmpTw4n

Initialization
"""

import os
import math
import sys
import nltk
#nltk.download('punkt')

from nltk.stem import SnowballStemmer 
from nltk.tokenize import word_tokenize, sent_tokenize

wordmap = {}
sentmap = {}
stemmer = SnowballStemmer("english")
docnum = 0

"""File opening for term frequency"""

def file_open(fl):
  with open(fl, 'r') as f:
     for line in f:
       words = word_tokenize(line)
       for w in words:
         term = stemmer.stem(w.lower())
         if (term not in wordmap and term.isalnum()):
           wordmap[term] = 1
         else:
           if(term.isalnum()):
            wordmap[term] += 1

"""Parsing of the input"""

directory, pth = input().split(" ")
path = os.path.split(pth)
title = path[-1]
os.chdir(path[0])
file_open(title)

"""Document frequency"""

def df_count(fl):
  with open(fl, 'r') as f:
    text = f.read()
    words = word_tokenize(text)
    
    for i in range(len(words)):
      words[i] = stemmer.stem(words[i].lower())
    for key in stems:
      if key in words:
        stems[key] += 1

"""Handling nested directories"""

def dir_tree(dir):
  global docnum
  stack = [dir]

  while stack:
    pushed = 0
    dir = stack.pop()
    files = os.listdir(dir)
    os.chdir(dir)
    for file in files:
      if not file.endswith(".txt"):
        pushed += 1
        stack.append(file)
        
      else:
        docnum += 1
        df_count(file)
    if not pushed:
      path_parent = os.path.dirname(os.getcwd())
      os.chdir(path_parent)

"""Preparation for counting DF"""

stems = {key:0 for key in wordmap}

for roots, dirs, files in os.walk(directory):
  for dir in dirs:
    os.chdir(directory)
    if not dir.startswith("."):
      dir_tree(dir)
  break

"""Conversion to IDF from DF"""

def to_idf(stems):
  for key in stems:
    stems[key] = math.log((docnum/stems[key]), 10)

"""TF-IDF """

def calculate_tfidf(wordmap, stems, result):
  wm = wordmap.copy()
  stm = stems.copy()
  for i in range(10):
    max = 0
    max_key = ""
    for key in stm:
      tf_idf = stm[key]*wm[key]
      if tf_idf > max:
        max = tf_idf
        max_key = key
    
    result.append([max_key, max])
    wm.pop(max_key, None)
    stm.pop(max_key, None)
    max = 0
    max_key = ""

"""Sorting some initial TF-IDF results"""

result = []
to_idf(stems)
calculate_tfidf(wordmap, stems, result)

for i in range(len(result)-1):
  for j in range(i, len(result)):
    if(result[i][-1] == result[j][-1]):
      if(result[i][0] > result[j][0]):
        tmp = result[i][0]
        result[i][0] = result[j][0]
        result[j][0] = tmp

"""TF-IDF of sentences"""

def sent_tfidf(words, sent):
  results = []
  wm = wordmap.copy()
  stm = stems.copy()
  added = []
  found = True
  for i in range(10):
    max = 0
    max_key = ""
    rmv = ""
    
    for word in words:

      wrd = stemmer.stem(word.lower())
      if wrd in added:
        max = stems[wrd]*wordmap[wrd]
        max_key = wrd
        rmv = word
        break
      else:
        tf_idf = stm[wrd]*wm[wrd]
        if tf_idf > max:
          max = tf_idf
          max_key = wrd
          rmv = word
    
    if rmv != "": 
      words.remove(rmv)
      added.append(max_key)
      results.append([max_key, max])
      wm.pop(max_key, None)
      stm.pop(max_key, None)
    max = 0
    max_key = ""
    rmv = ""

  stfidf = 0
  for res in results:
    stfidf += res[-1]
  sentmap[sent] = stfidf

"""Preparation for sentence tf-idf calculation"""

def second_task(fl):
  
  sts = []
  with open(fl, 'r') as f:
    content = f.read()
    sentences = sent_tokenize(content)
    sts = list(sentences)
    for sent in sentences:
      words = word_tokenize(sent)
      words = [word for word in words if word.isalnum()]
      sent_tfidf(words, sent)
  
  return sts, dict(sorted(sentmap.items(), key = lambda item: item[1], reverse = True))

"""Sorting and printing out final data"""

terms = []
for i in range(len(result)):
  terms.append(result[i][0])
os.chdir(path[0])
sts, secondmap = second_task(title)

sents = []
sentences = []
for key in secondmap.keys():
  sents.append(key)
  
for i in range(5):
  sentences.append(sents[i])
for i in range(len(sentences)-1):
  for j in range(i, len(sentences)):
    if(secondmap[sentences[i]] == secondmap[sentences[j]]):
      if(sts.index(sentences[i]) > sts.index(sentences[j])):
        tmp = sentences[i]
        sentences[i] = sentences[j]
        sentences[j] = tmp
print(*terms, sep = ", ")
print(*sentences, sep = " ")